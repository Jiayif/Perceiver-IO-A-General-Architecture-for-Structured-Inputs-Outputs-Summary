{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perceiver for text"
      ],
      "metadata": {
        "id": "ZUiGLy5HloEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0hBU3Bplirs"
      },
      "outputs": [],
      "source": [
        "from transformers import PerceiverTokenizer\n",
        "\n",
        "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n",
        "\n",
        "text = \"hello world\"\n",
        "\n",
        "inputs = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\").input_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceiver model for text classification (which is called PerceiverForSequenceClassification in HuggingFace Transformers):\n",
        "from torch import nn\n",
        "from transformers import PerceiverModel\n",
        "from transformers.models.perceiver.modeling_perceiver import PerceiverTextPreprocessor, PerceiverClassificationDecoder\n",
        "\n",
        "class PerceiverForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.perceiver = PerceiverModel(\n",
        "            config,\n",
        "            input_preprocessor=PerceiverTextPreprocessor(config),\n",
        "            decoder=PerceiverClassificationDecoder(\n",
        "                config,\n",
        "                num_channels=config.d_latents,\n",
        "                trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
        "                use_query_residual=True,\n",
        "            ),\n",
        "        )"
      ],
      "metadata": {
        "id": "vfUXgYaqlq1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At initialization, PerceiverModel internally defines a set of latent variables, as follows:\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "self.latents = nn.Parameter(torch.randn(config.num_latents, config.d_latents))"
      ],
      "metadata": {
        "id": "W9oHwCqfl2MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceiver for images\n",
        "\n"
      ],
      "metadata": {
        "id": "atN-_L00lrPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import PerceiverModel\n",
        "from transformers.models.perceiver.modeling_perceiver import PerceiverImagePreprocessor, PerceiverClassificationDecoder\n",
        "\n",
        "class PerceiverForImageClassificationLearned(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.perceiver = PerceiverModel(\n",
        "            config,\n",
        "            input_preprocessor=PerceiverImagePreprocessor(\n",
        "                config,\n",
        "                prep_type=\"conv1x1\",\n",
        "                spatial_downsample=1,\n",
        "                out_channels=256,\n",
        "                position_encoding_type=\"trainable\",\n",
        "                concat_or_add_pos=\"concat\",\n",
        "                project_pos_dim=256,\n",
        "                trainable_position_encoding_kwargs=dict(num_channels=256, index_dims=config.image_size ** 2),\n",
        "            ),\n",
        "            decoder=PerceiverClassificationDecoder(\n",
        "                config,\n",
        "                num_channels=config.d_latents,\n",
        "                trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
        "                use_query_residual=True,\n",
        "            ),\n",
        "        )"
      ],
      "metadata": {
        "id": "uzMtdEp3mkpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PerceiverFeatureExtractor\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "feature_extractor = PerceiverFeatureExtractor.from_pretrained(\"deepmind/vision-perceiver\")\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = feature_extractor(image, return_tensors=\"pt\").pixel_values"
      ],
      "metadata": {
        "id": "xB_tbBhqmpAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceiver for optical flow"
      ],
      "metadata": {
        "id": "q8AenAfMmuRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import PerceiverModel\n",
        "from transformers.models.perceiver.modeling_perceiver import PerceiverImagePreprocessor, PerceiverOpticalFlowDecoder\n",
        "\n",
        "class PerceiverForOpticalFlow(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        fourier_position_encoding_kwargs_preprocessor = dict(\n",
        "            num_bands=64,\n",
        "            max_resolution=config.train_size,\n",
        "            sine_only=False,\n",
        "            concat_pos=True,\n",
        "        )\n",
        "        fourier_position_encoding_kwargs_decoder = dict(\n",
        "            concat_pos=True, max_resolution=config.train_size, num_bands=64, sine_only=False\n",
        "        )\n",
        "        \n",
        "        image_preprocessor = PerceiverImagePreprocessor(\n",
        "            config,\n",
        "            prep_type=\"patches\",\n",
        "            spatial_downsample=1,\n",
        "            conv_after_patching=True,\n",
        "            conv_after_patching_in_channels=54,\n",
        "            temporal_downsample=2,\n",
        "            position_encoding_type=\"fourier\",\n",
        "            # position_encoding_kwargs\n",
        "            fourier_position_encoding_kwargs=fourier_position_encoding_kwargs_preprocessor,\n",
        "        )\n",
        "        \n",
        "        self.perceiver = PerceiverModel(\n",
        "            config,\n",
        "            input_preprocessor=image_preprocessor,\n",
        "            decoder=PerceiverOpticalFlowDecoder(\n",
        "                config,\n",
        "                num_channels=image_preprocessor.num_channels,\n",
        "                output_image_shape=config.train_size,\n",
        "                rescale_factor=100.0,\n",
        "                use_query_residual=False,\n",
        "                output_num_channels=2,\n",
        "                position_encoding_type=\"fourier\",\n",
        "                fourier_position_encoding_kwargs=fourier_position_encoding_kwargs_decoder,\n",
        "            ),\n",
        "        )"
      ],
      "metadata": {
        "id": "zXWiv3zamvHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}